{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ghi dữ liệu ra Kafka\n",
    "\n",
    "Ở phần này, ta sẽ miêu tả việc hỗ trợ cho ghi Truy vấn trực tiếp theo dòng và Tập hợp truy vấn ra Apache Kafka. **Nhớ rằng Apache Kafka chỉ đảm bảo việc ghi ít nhất một lần**. Hệ quả là, khi viết, dù là Truy vấn trực tiếp (Streaming Queries) hay Tập hợp Truy vấn (Batch Queries) đến Kafka, một số bản ghi có thể bị trùng lặp. Điều này có thể xảy ra, ví dụ, khi Kafka muốn gửi lại một tin nhắn mà không được xác nhận bởi Broker, mặc dù Broker đã nhận và đã ghi lại tin nhắn đó. Cấu trúc truyền trực tiếp không thể ngăn được sự trùng lặp này diễn ra do cách Kafka viết. Tuy nhiên, nếu quá trình ghi lại truy vấn diễn ra thành công, ta có thể giả định là kết quả của truy vấn được viết ít nhất một lần. Một giải pháp khả thi để loại bỏ việc trùng lặp này là khi đọc dữ liệu có thể sử dụng một khoá chính để tránh trùng lặp khi đọc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataframe viết vào Kafka sẽ có cấu trúc các cột như sau\n",
    "\n",
    "|                         Column                    |                                  Type                                   |\n",
    "| ------------------------------------------------- | ----------------------------------------------------------------------- |\n",
    "| Khoá (key) (tuỳ chọn)                             | string hoặc binary                                                      |\n",
    "| Giá trị (value) (bắt buộc)                        | string hoặc binary                                                      |\n",
    "| Phần đầu (Headers) (tuỳ chọn)                     | mảng                                                                    |\n",
    "| Chủ đề (Topic) (tuỳ chọn)                         | string                                                                  |\n",
    "| Phân vùng (Partition) (tuỳ chọn)                  | int                                                                     |\n",
    "\n",
    "**Lưu ý: Với trường topic, nếu trong cấu hình chưa được chỉ ra thì trường này phải bắt buộc được chỉ rõ ở trong dataframe**\n",
    "\n",
    "Cột giá trị là cột duy nhất bắt buộc phải có. Nếu cột khoá không được chỉ ra thì khoá sẽ được gán giá trị \"null\" một cách tự động (xem cách Kafka viết (Kafka semantics) để hiểu hơn cách null được xử lý). Nếu cột chủ đề tồn tại thì giá trị của nó được sử dụng như chủ đề khi viết vào một dòng (row) đến Kafka, trừ khi cấu hình \"topic\" đã được thiết lập. Tức là cấu hình \"topic\" sẽ ghi đè lên cột topic. Nếu cột \"partition\" khôn được chỉ ra (hoặc có nhưng giá trị là null) thì phân vùng sẽ được tính toán và xác định bởi Kafka producer. Một phân vùng Kafka có thể được chỉ định bởi Spark trong cài đặt tuỳ chọn của kafka.partitioner.class. Nếu không được nói đến, phân vùng Kafka mặc định sẽ được sử dụng."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Các tuỳ chọn sau phải được sử dụng cho Kafka sink cho cả truy vấn tập hợp và trực tiếp theo dòng:\n",
    "    \n",
    "| Tuỳ chọn | Giá trị | Ý nghĩa |\n",
    "| :---------------------- | :----------------------------- | :--------------------------------------------------------------- |\n",
    "| kafka.bootstrap.servers | Một danh sách \"host:port\" phân cách bởi dấu phẩy | Cấu hình server Kafka \"bootstrap.servers\"      |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Các trường cấu hình sau là tuỳ chọn:\n",
    "    \n",
    "| Tuỳ chọn | Giá trị | Mặc định | Kiểu truy vấn | Ý nghĩa |\n",
    "| :------- | :------ | :------- | :------------------ | :-------------------------------------------------------------------- |\n",
    "| Chủ đề (topic) | string | none | Trực tiếp và tập hợp | Thiết lập chủ đề mà tất cả các dòng sẽ được ghi vào trong Kafka. Trường này khi được thiết lập sẽ ghi đè lên tất cả các cột topic (nếu có) trong dữ liệu. |\n",
    "| includeHeaders (bao gồm phần đầu) | boolean | false |  Trực tiếp và tập hợp | Khi muốn bao gồm phần đầu trong các dòng dữ liệu |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: kafka-python in /usr/local/lib/python3.7/dist-packages (2.0.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install kafka-python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "để có thể sử dụng kafka thì chúng ta cần khai báo một số dòng lệnh như phía dưới"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "packages = \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.0.1\"\n",
    "\n",
    "os.environ[\"PYSPARK_SUBMIT_ARGS\"] = (\n",
    "    \"--packages {0} pyspark-shell\".format(packages)\n",
    ")\n",
    "\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql.session import SparkSession\n",
    "from pyspark.sql.functions import to_json, struct, lit\n",
    "sc = SparkContext('local')\n",
    "spark = SparkSession(sc)\n",
    "\n",
    "KAFKA_BROKER = \"kafka:9092\"\n",
    "KAFKA_TOPIC = \"hello\"\n",
    "\n",
    "#KAFKA_TOPIC=\"table1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "tiến hành đọc file csv và in ra thử data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----------------+--------------------+-------------+------------------+--------+----------+-----+-------+-------+---------+------------------+-------------+\n",
      "|androidVersion|        category|            comments|contentRating|    currentVersion|installs|lastUpdate|price|ratings|reviews|    score|              size|        title|\n",
      "+--------------+----------------+--------------------+-------------+------------------+--------+----------+-----+-------+-------+---------+------------------+-------------+\n",
      "|           4.1|   Entertainment|[Fire your UX dep...|         Teen|             4.4.2|  500000|1601747961|  0.0|   5552|   2069|3.2570977|              8.4M|        RTL24|\n",
      "|        Varies|        Shopping|[Objednávání jídl...|     Everyone|Varies with device|  100000|1587063918|  0.0|   2671|    610|3.8988764|Varies with device|    Strava.cz|\n",
      "|        Varies|   Communication|[I've been using ...|     Everyone|Varies with device| 1000000|1594104671|  0.0|   9143|   2126|4.0847826|Varies with device|     ABV Mail|\n",
      "|           4.4|         Weather|[Jól működik, Az ...|     Everyone|             3.2.0|   10000|1568716901|  0.0|    240|     70|     3.94|              3.7M|      TAVIHAR|\n",
      "|           4.3|Health & Fitness|                  []| Everyone 10+|            1.0.18|    5000|1530602291|  0.0|     31|     18|2.7419355|              7.0M|Életmódváltók|\n",
      "+--------------+----------------+--------------------+-------------+------------------+--------+----------+-----+-------+-------+---------+------------------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df=spark.read.format(\"json\").option(\"mode\", \"FAILFAST\")\\\n",
    ".option(\"inferSchema\", \"true\").load(\"hdfs://namenode/user/root/data25000-30000.json\")        \n",
    "# df_list = df.collect()\n",
    "# print(df_list[0])\n",
    "# row_dict = df_list[0].asDict()\n",
    "# print(row_dict)\n",
    "df.show(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ghi đầu ra của truy vấn vào kafka\n",
    "có 2 cách để có thể ghi dữ liệu từ dataframe đến kafka\n",
    "1. sử dụng KafkaProducer\n",
    "2. sử dụng write của spark\n",
    "\n",
    "ví dụ dưới mô tả cách sử dụng KafkaProducer để ghi dữ liệu từ dataframe lên kafka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-cb4aee08ef70>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mrow_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masDict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     future = producer.send(\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import json\n",
    "import random\n",
    "import logging\n",
    "\n",
    "from kafka import KafkaProducer\n",
    "from kafka.errors import KafkaError\n",
    "\n",
    "producer = KafkaProducer(bootstrap_servers=[KAFKA_BROKER])\n",
    "index = 0\n",
    "\n",
    "while True:\n",
    "    \n",
    "    row_dict = df_list[index].asDict()\n",
    "    \n",
    "    future = producer.send(\n",
    "        topic=KAFKA_TOPIC, \n",
    "        key=str(row_dict[\"_c0\"]).encode(\"utf-8\"),\n",
    "        value=json.dumps(row_dict).encode(\"utf-8\"))\n",
    "    \n",
    "    try:\n",
    "        record_metadata = future.get(timeout=10)\n",
    "    except KafkaError:\n",
    "        # Decide what to do if produce request failed...\n",
    "        logging.exception(\"Error\")\n",
    "        pass\n",
    "    \n",
    "    producer.flush()\n",
    "    \n",
    "    index += 1\n",
    "    time.sleep(random.uniform(0.1,3.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ví dụ tiếp theo mô tả cách ghi dữ liệu lên kafka sử dung hàm write của spark\n",
    "\n",
    "do kafka chỉ hỗ trợ gửi dữ liệu định dạng key:value nên ta cần phải chỉnh sửa dữ liệu trước khi gửi\n",
    "ở đây ta tạo 1 cột mới có tên value với dữ liệu là dữ liệu của từng hàng trong dataframe với định dạng json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------------------+\n",
      "|          key|             valuess|\n",
      "+-------------+--------------------+\n",
      "|        RTL24|{\"androidVersion\"...|\n",
      "|    Strava.cz|{\"androidVersion\"...|\n",
      "|     ABV Mail|{\"androidVersion\"...|\n",
      "|      TAVIHAR|{\"androidVersion\"...|\n",
      "|Életmódváltók|{\"androidVersion\"...|\n",
      "+-------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = df.withColumn('value' ,to_json(struct([df[x] for x in df.columns])))\n",
    "df.selectExpr(\"CAST(title AS STRING) as key\", \"CAST(value AS STRING) as valuess\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "để có thể gửi dữ liệu từ dataframe tới kafka ta cần ít nhất 2 cột, 1 cột key và 1 cột value. \n",
    "1. sử dụng format(\"kafka\") để dịnh nghĩa việc gửi dữ liệu đến kafka\n",
    "2. sử dụng option(\"kafka.bootstrap.servers\", KAFKA_BROKER) để định nghĩa broker kafka đích, có thể định nghĩa nhiều broker dích ngăn cách nhau bới dấu ',' ví dụ \"host1:port1,host2:port2\"\n",
    "3. sử dụng option(\"topic\", KAFKA_TOPIC) để định nghĩa topic đích trong broker dích\n",
    "\n",
    "ở đây ta chọn cột \\_c0 là cột key và cột value chưa toàn bộ dữ liệu định dạng json ta vừa tạo ở trên"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.selectExpr(\"CAST(title AS STRING) as key\", \"CAST(value AS STRING)\")\\\n",
    "  .write\\\n",
    "  .format(\"kafka\")\\\n",
    "  .option(\"kafka.bootstrap.servers\", KAFKA_BROKER)\\\n",
    "  .option(\"topic\", KAFKA_TOPIC)\\\n",
    "  .save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ngoài cách định nghĩa 2 cột 1 cột key, 1 cột value thì ta cũng có thể sử dụng 1 cột trong dataframe để định nghĩa topic dích thay cho việc phải khai báo option topic gửi đến\n",
    "\n",
    "dưới đây ta tạo 1 cột mới có tên là topic với toàn bộ dữ liệu là tên topic được định nghĩa trong biến KAFKA_TOPIC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+\n",
      "|topic|                 key|               value|\n",
      "+-----+--------------------+--------------------+\n",
      "|hello|               RTL24|{\"androidVersion\"...|\n",
      "|hello|           Strava.cz|{\"androidVersion\"...|\n",
      "|hello|            ABV Mail|{\"androidVersion\"...|\n",
      "|hello|             TAVIHAR|{\"androidVersion\"...|\n",
      "|hello|       Életmódváltók|{\"androidVersion\"...|\n",
      "|hello|Nosalty - A főzés...|{\"androidVersion\"...|\n",
      "|hello|Prague Visitor Guide|{\"androidVersion\"...|\n",
      "|hello|               Vbox7|{\"androidVersion\"...|\n",
      "|hello|           RSOE-EDIS|{\"androidVersion\"...|\n",
      "|hello|           PannonRIS|{\"androidVersion\"...|\n",
      "+-----+--------------------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = df.withColumn('topic' ,lit(KAFKA_TOPIC))\n",
    "df.selectExpr('topic', \"CAST(title AS STRING) as key\", \"CAST(value AS STRING)\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|                 key|               value|\n",
      "+--------------------+--------------------+\n",
      "|FeelingU-Live Vid...|{\"androidVersion\"...|\n",
      "|Tick - Chat all o...|{\"androidVersion\"...|\n",
      "|Vico - Video Chat...|{\"androidVersion\"...|\n",
      "|Meet Strangers & ...|{\"androidVersion\"...|\n",
      "|Vistar - Live Vid...|{\"androidVersion\"...|\n",
      "|Peachat - Live Vi...|{\"androidVersion\"...|\n",
      "|Meetbay - Live Ch...|{\"androidVersion\"...|\n",
      "|Mico Cora - live ...|{\"androidVersion\"...|\n",
      "|Group Voice Chat ...|{\"androidVersion\"...|\n",
      "|Aussie Mingle - A...|{\"androidVersion\"...|\n",
      "|KoreanCupid - Kor...|{\"androidVersion\"...|\n",
      "|Tomato - live vid...|{\"androidVersion\"...|\n",
      "|Helwa-Live Chat O...|{\"androidVersion\"...|\n",
      "|           SugarTime|{\"androidVersion\"...|\n",
      "|OE Match - Date, ...|{\"androidVersion\"...|\n",
      "|Love Quotes, Dail...|{\"androidVersion\"...|\n",
      "|Kmate-Meet Korean...|{\"androidVersion\"...|\n",
      "|Hoogo: Meet New P...|{\"androidVersion\"...|\n",
      "|ARGO - Social Vid...|{\"androidVersion\"...|\n",
      "|EME Hive - Premiu...|{\"androidVersion\"...|\n",
      "+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark\\\n",
    ".read\\\n",
    ".format(\"kafka\")\\\n",
    ".option(\"kafka.bootstrap.servers\", \"kafka:9092\")\\\n",
    ".option(\"subscribe\", \"hello\")\\\n",
    ".load()\n",
    "\n",
    "df.selectExpr(\"CAST(key AS STRING)\", \"CAST(value AS STRING)\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ví dụ dưới sử dụng cột topic vừa tạo để định nghĩa topic gửi đến thay vì sử dụng option như ví dụ phía trên"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "cannot resolve '`_c0`' given input columns: [key, offset, partition, timestamp, timestampType, topic, value]; line 1 pos 5;\n'Project [topic#217, cast('_c0 as string) AS key#264, cast(value#216 as string) AS value#265]\n+- Relation[key#215,value#216,topic#217,partition#218,offset#219L,timestamp#220,timestampType#221] KafkaRelation(strategy=Subscribe[taipt], start=EarliestOffsetRangeLimit, end=LatestOffsetRangeLimit)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-15a5599a63e2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselectExpr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'topic'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"CAST(_c0 AS STRING) as key\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"CAST(value AS STRING)\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m   \u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m   \u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"kafka\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"kafka.bootstrap.servers\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mKAFKA_BROKER\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mselectExpr\u001b[0;34m(self, *expr)\u001b[0m\n\u001b[1;32m   1433\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexpr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexpr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1434\u001b[0m             \u001b[0mexpr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexpr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1435\u001b[0;31m         \u001b[0mjdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselectExpr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jseq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexpr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1436\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql_ctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1437\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1305\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1307\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    135\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m                 \u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconverted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    138\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(e)\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: cannot resolve '`_c0`' given input columns: [key, offset, partition, timestamp, timestampType, topic, value]; line 1 pos 5;\n'Project [topic#217, cast('_c0 as string) AS key#264, cast(value#216 as string) AS value#265]\n+- Relation[key#215,value#216,topic#217,partition#218,offset#219L,timestamp#220,timestampType#221] KafkaRelation(strategy=Subscribe[taipt], start=EarliestOffsetRangeLimit, end=LatestOffsetRangeLimit)\n"
     ]
    }
   ],
   "source": [
    "df.selectExpr('topic',\"CAST(_c0 AS STRING) as key\", \"CAST(value AS STRING)\")\\\n",
    "  .write\\\n",
    "  .format(\"kafka\")\\\n",
    "  .option(\"kafka.bootstrap.servers\", KAFKA_BROKER)\\\n",
    "  .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column<b'default_topic'>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(lit(KAFKA_TOPIC))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tạo kafka sink để truyền stream dữ liệu\n",
    "ngoài việc truyền dữ liệu từ truy vấn thì ta cũng có thể stream dữ liệu từ streaming dataframe lên kafka\n",
    "\n",
    "trước khi có thể stream dữ liệu thì ta cần có stream dataframe \n",
    "việc tạo stream dataframe tương tự như trong **Chapter 21: Structured Streaming Basics**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.json(\"../data/activity-data/\")\n",
    "dataSchema = df.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=spark.read.format(\"json\").option(\"mode\", \"FAILFAST\")\\\n",
    ".option(\"inferSchema\", \"true\").load(\"../data/activity-data\")        \n",
    "dataSchema = df.schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "đọc dữ liệu từ folder và lấy schema để có thể tạo stream dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'static' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-dcd5c383f579>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mstatic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprintSchema\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mstatic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'static' is not defined"
     ]
    }
   ],
   "source": [
    "df.printSchema()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StructType(List(StructField(Arrival_Time,LongType,true),StructField(Creation_Time,LongType,true),StructField(Device,StringType,true),StructField(Index,LongType,true),StructField(Model,StringType,true),StructField(User,StringType,true),StructField(gt,StringType,true),StructField(x,DoubleType,true),StructField(y,DoubleType,true),StructField(z,DoubleType,true)))\n"
     ]
    }
   ],
   "source": [
    "dataSchema = static.schema\n",
    "print(dataSchema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------------------+--------+-----+------+----+-----+------------+------------+------------+\n",
      "| Arrival_Time|      Creation_Time|  Device|Index| Model|User|   gt|           x|           y|           z|\n",
      "+-------------+-------------------+--------+-----+------+----+-----+------------+------------+------------+\n",
      "|1424686735090|1424686733090638193|nexus4_1|   18|nexus4|   g|stand| 3.356934E-4|-5.645752E-4|-0.018814087|\n",
      "|1424686735292|1424688581345918092|nexus4_2|   66|nexus4|   g|stand|-0.005722046| 0.029083252| 0.005569458|\n",
      "|1424686735500|1424686733498505625|nexus4_1|   99|nexus4|   g|stand|   0.0078125|-0.017654419| 0.010025024|\n",
      "|1424686735691|1424688581745026978|nexus4_2|  145|nexus4|   g|stand|-3.814697E-4|   0.0184021|-0.013656616|\n",
      "+-------------+-------------------+--------+-----+------+----+-----+------------+------------+------------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "static.show(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tạo stream dataframe với schema đọc được, tối đa 1 file mốĩ lần trigger từ folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "streaming = spark.readStream.schema(dataSchema).option(\"maxFilesPerTrigger\", 1)\\\n",
    ".json(\"../data/activity-data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cũng tương tự như việc gửi dữ liệu query thì việc gửi dữ liệu stream cũng cần tối thiểu 2 cột key và value\n",
    "1. sử dụng writeStream để ghi stream dataframe lên kafka\n",
    "2. sử dụng option(\"kafka.bootstrap.servers\", KAFKA_BROKER) để định nghĩa broker đích, có thể định nghĩa nhiều broker dích ngăn cách nhau bới dấu ',' ví dụ \"host1:port1,host2:port2\"\n",
    "3. sử dụng option(\"topic\", KAFKA_TOPIC) để định nghĩa topic đích hoặc cos thể thay thế bằng việc định nghĩa cột topic trong dataframe\n",
    "4. sử dụng option(\"checkpointLocation\", \"checkpoint\") để lưu checkpoint quá trình stream data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ví dụ dưới tạo 1 cột mới có tên là value có dữ liệu là dữ liệu của từng hàng với định dạng json sau đó stream lên kafka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.streaming.StreamingQuery at 0x7ff912564da0>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "streaming.withColumn('value' ,to_json(struct([streaming[x] for x in streaming.columns])))\\\n",
    "  .selectExpr(\"CAST(Arrival_Time AS STRING) as key\", \"CAST(value AS STRING)\") \\\n",
    "  .writeStream \\\n",
    "  .format(\"kafka\") \\\n",
    "  .option(\"kafka.bootstrap.servers\", KAFKA_BROKER) \\\n",
    "  .option(\"topic\", KAFKA_TOPIC) \\\n",
    "  .option(\"checkpointLocation\", \"checkpoint\")\\\n",
    "  .start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ví dụ dưới tạo 1 cột topic và điền toàn bộ giá trị là là trị được định nghĩa trong biến KAFKA_TOPIC sau đó stream lên kafka sử dụng 3 cột topic, key, value thay cho việc sử dụng option để định nghĩa topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.streaming.StreamingQuery at 0x7ff912dc8908>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "streaming.withColumn('topic' ,lit(KAFKA_TOPIC))\\\n",
    "  .withColumn('value' ,to_json(struct([streaming[x] for x in streaming.columns])))\\\n",
    "  .selectExpr('topic', \"CAST(Arrival_Time AS STRING) as key\", \"CAST(value AS STRING)\") \\\n",
    "  .writeStream \\\n",
    "  .format(\"kafka\") \\\n",
    "  .option(\"kafka.bootstrap.servers\", KAFKA_BROKER) \\\n",
    "  .option(\"checkpointLocation\", \"checkpoint\")\\\n",
    "  .start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Producer caching\n",
    "do thực thể kafka producer được thiết kế để an toàn cho luồng, spark khởi tạo thực thể kafka producer và đồng sử dụng giữa các tác vụ với cùng caching key\n",
    "\n",
    "caching key được xây dựng từ thông tin sau\n",
    "- cấu hình kafka producer\n",
    "\n",
    "nó bao gồm cấu hình để ủy quyền, spark sẽ tự động bao gồm khi token ủy quyền được sử dụng. ngay cả khi chúng ta lấy ủy quyền vào account, chúng ta có thể mong đợi rằng cùng thực thể kafka producer được sử dụng với cùng cấu hình kafka producer. nó sẽ dùng thực thể kafka producer mới nếu token ủy quyền được làm mới. thực thể kafka producer với token ủy quyền cũ sẽ bị loại bỏ theo chính sách bộ nhớ đệm. các thuộc tính sau có sẵn để cấu hình nhóm producer:\n",
    "\n",
    "| tên thuộc tính | Mặc định | ý nghĩa | kể từ phiên bản|\n",
    "| :--- | :--- |:--- | :--- |\n",
    "| spark.kafka.producer.cache.timeout | 10 phút | khoảng thời gian nhỏ nhất mà 1 Producer không hoạt động trong pool trước khi bị loại bỏ | 2.2.1|\n",
    "|spark.kafka.producer.cache.evictorThreadRunInterval| 1 phút| khoảng thời gian dừng giữa các lần chạy của luồng evictor cho nhóm producer. Khi được được thiết lập là non-positive, không có luồng evictor nào được chạy.| 3.0.1|\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
